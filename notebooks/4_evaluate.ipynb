{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43dcaa59",
   "metadata": {},
   "source": [
    "# 4. Model Evaluation\n",
    "\n",
    "This notebook evaluates trained models using:\n",
    "- **COCO mAP** metrics (AP, AP50, AP75, AR).\n",
    "- **Colony counting** accuracy (AE, sAPE, per-class MAE).\n",
    "- **Visual inspection** of predictions on test images.\n",
    "\n",
    "**Prerequisites:** Run `1_setup.ipynb` and have trained models available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0109bd03",
   "metadata": {},
   "source": [
    "## 4.1 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8852a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "import config\n",
    "from utils.evaluation import evaluate_model, evaluate_bbox_counting, plot_training_curves\n",
    "from utils.visualization import show_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b78f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== CONFIGURE YOUR EVALUATION =====================\n",
    "\n",
    "# --- Dataset ---\n",
    "DATASET_SOURCE = \"agar\"        # 'agar' or 'roboflow'\n",
    "SUBSET = \"total\"               # For AGAR: 'total', 'bright', 'dark', 'vague', 'lowres'\n",
    "\n",
    "# --- Model to evaluate ---\n",
    "# Use a key from config.AGAR_TRAINED_MODELS or config.ROBOFLOW_TRAINED_MODELS\n",
    "TRAINED_MODEL_KEY = \"total_faster_rcnn_R101\"\n",
    "MODEL_SOURCE = \"agar\"          # 'agar' or 'roboflow'\n",
    "\n",
    "# --- Model architecture (must match the trained model) ---\n",
    "MODEL_KEY = \"faster_rcnn_R101\" # Must match the architecture of TRAINED_MODEL_KEY\n",
    "\n",
    "# --- Evaluation params ---\n",
    "NUM_CLASSES = 3                # AGAR: 3, Roboflow: 4\n",
    "SCORE_THRESHOLD = 0.5\n",
    "MAX_DETECTIONS = 100\n",
    "\n",
    "# ====================================================================\n",
    "\n",
    "print(f\"Evaluating: {TRAINED_MODEL_KEY} on {DATASET_SOURCE}/{SUBSET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5c1c4d",
   "metadata": {},
   "source": [
    "## 4.2 Dataset & Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c38711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset registration ---\n",
    "if DATASET_SOURCE == \"agar\":\n",
    "    dataset = config.AGAR_DATASETS[SUBSET]\n",
    "    test_path = dataset[\"test\"]\n",
    "    img_dir_test = config.AGAR_IMG_DIR\n",
    "    test_name = f\"{SUBSET}_test\"\n",
    "elif DATASET_SOURCE == \"roboflow\":\n",
    "    dataset = config.ROBOFLOW_DATASETS[\"curated\"]\n",
    "    test_path = dataset[\"test\"]\n",
    "    img_dir_test = dataset[\"test_dir\"]\n",
    "    test_name = \"robo_test\"\n",
    "\n",
    "if test_name in DatasetCatalog.list():\n",
    "    DatasetCatalog.remove(test_name)\n",
    "    MetadataCatalog.remove(test_name)\n",
    "register_coco_instances(test_name, {}, test_path, img_dir_test)\n",
    "\n",
    "# --- Build predictor ---\n",
    "config_file = config.MODELS[MODEL_KEY]\n",
    "model_weights = config.get_model_weights(TRAINED_MODEL_KEY, MODEL_SOURCE)\n",
    "\n",
    "output_dir = os.path.dirname(model_weights)\n",
    "eval_output = os.path.join(output_dir, \"test\")\n",
    "os.makedirs(eval_output, exist_ok=True)\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(config_file))\n",
    "cfg.DATASETS.TEST = (test_name,)\n",
    "cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = False\n",
    "cfg.MODEL.WEIGHTS = model_weights\n",
    "cfg.TEST.DETECTIONS_PER_IMAGE = MAX_DETECTIONS\n",
    "\n",
    "if config.is_retinanet(MODEL_KEY):\n",
    "    cfg.MODEL.RETINANET.NUM_CLASSES = NUM_CLASSES\n",
    "    cfg.MODEL.RETINANET.SCORE_THRESH_TEST = SCORE_THRESHOLD\n",
    "else:\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASSES\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = SCORE_THRESHOLD\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "print(f\"Model loaded from: {model_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb691628",
   "metadata": {},
   "source": [
    "## 4.3 COCO mAP Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cda5328",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_model(cfg, predictor, test_name, eval_output, max_dets=MAX_DETECTIONS)\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a87b16e",
   "metadata": {},
   "source": [
    "## 4.4 Colony Counting Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c8fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_path = os.path.join(eval_output, \"coco_instances_results.json\")\n",
    "\n",
    "if os.path.exists(predictions_path):\n",
    "    df = evaluate_bbox_counting(\n",
    "        annotations_file=test_path,\n",
    "        predictions_file=predictions_path,\n",
    "        score_threshold=SCORE_THRESHOLD,\n",
    "        iou_threshold=0.5,\n",
    "        output_dir=eval_output,\n",
    "        num_classes=NUM_CLASSES,\n",
    "    )\n",
    "    print(f\"\\nMean Absolute Error: {df['AE'].mean():.2f}\")\n",
    "    print(f\"Mean sAPE: {df['sAPE'].mean():.4f}\")\n",
    "    display(df.head(10))\n",
    "else:\n",
    "    print(f\"Predictions file not found at {predictions_path}\")\n",
    "    print(\"Run the COCO evaluation cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe01ef9",
   "metadata": {},
   "source": [
    "## 4.5 Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d49de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(predictor, test_name, num_samples=5, scale=0.5, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe79f37",
   "metadata": {},
   "source": [
    "## 4.6 Training Curves (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd90a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path = os.path.join(output_dir, \"metrics.json\")\n",
    "if os.path.exists(metrics_path):\n",
    "    plot_training_curves(metrics_path)\n",
    "else:\n",
    "    print(f\"No metrics.json found at {metrics_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
