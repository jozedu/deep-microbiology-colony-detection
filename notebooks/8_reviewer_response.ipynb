{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc10741",
   "metadata": {},
   "source": [
    "# 8. Reviewer Response — Reproducibility & Statistical Analysis\n",
    "\n",
    "This notebook addresses the following reviewer concerns:\n",
    "\n",
    "1. **[A] Missing full hyperparameters** — Dump the fully resolved Detectron2 config\n",
    "   (anchors, augmentation, normalization, input sizes, etc.) for each model.\n",
    "2. **[B] No uncertainty quantification** — Bootstrap confidence intervals on\n",
    "   existing test-set predictions.\n",
    "3. **[C] COCO mAP thresholds not justified for small colonies** — Evaluate at\n",
    "   multiple IoU thresholds and analyse colony size distribution.\n",
    "4. **[D] Summary** — Auto-generated summary for the reviewer response letter.\n",
    "5. **[E] Filter sensitivity** — Quantify the impact of the >100 annotation\n",
    "   threshold on data distribution and results.\n",
    "6. **[F] Multi-seed & YOLOv8 integration** — Load results from notebooks 10/11\n",
    "   to report training variance and cross-architecture comparison.\n",
    "\n",
    "**Prerequisites:** Run `1_setup.ipynb`. For Parts E–F, also run notebooks 10 and 11."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb02a345",
   "metadata": {},
   "source": [
    "## 8.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import config\n",
    "from utils.reproducibility import (\n",
    "    dump_full_config,\n",
    "    extract_key_config_summary,\n",
    "    generate_reproducibility_report,\n",
    "    print_config_summary,\n",
    ")\n",
    "from utils.evaluation import (\n",
    "    bootstrap_coco_eval,\n",
    "    format_ci_table,\n",
    "    plot_bootstrap_distributions,\n",
    "    multi_threshold_evaluate,\n",
    "    format_multi_threshold_table,\n",
    "    size_distribution_analysis,\n",
    "    filter_sensitivity_analysis,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343aa4f",
   "metadata": {},
   "source": [
    "---\n",
    "## Part A: Full Hyperparameter & Config Documentation\n",
    "\n",
    "**Reviewer concern:** *\"Missing full hyperparameters, anchors, normalization,\n",
    "random seeds, augmentation details. Missing exact Detectron2 config files.\"*\n",
    "\n",
    "**Response:** We reconstruct the fully resolved Detectron2 config for each\n",
    "model architecture. All training used the Detectron2 model zoo defaults;\n",
    "only the parameters listed in our thesis (LR, batch size, epochs, etc.)\n",
    "were overridden. The full resolved configs are exported as YAML files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b463cd",
   "metadata": {},
   "source": [
    "### A.1 Generate Config Summary for One Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c31ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a representative model\n",
    "MODEL_KEY = \"faster_rcnn_R101\"\n",
    "NUM_CLASSES = 3  # AGAR dataset\n",
    "\n",
    "summary = extract_key_config_summary(MODEL_KEY, num_classes=NUM_CLASSES)\n",
    "print_config_summary(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a96d413",
   "metadata": {},
   "source": [
    "### A.2 Export Full Resolved Config (YAML) for All Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88374501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export for all 6 architectures used in the study\n",
    "EXPORT_DIR = os.path.join(config.RESULTS_DIR, \"reproducibility\")\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "for model_key in config.MODELS:\n",
    "    yaml_path = os.path.join(EXPORT_DIR, f\"{model_key}_full_config.yaml\")\n",
    "    dump_full_config(model_key, num_classes=NUM_CLASSES, output_path=yaml_path)\n",
    "\n",
    "print(f\"\\n✓ All configs exported to {EXPORT_DIR}\")\n",
    "print(\"  Files:\", os.listdir(EXPORT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe26df1f",
   "metadata": {},
   "source": [
    "### A.3 Export Per-Trained-Model Reports\n",
    "\n",
    "For each trained model directory, save the full config + summary alongside the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c6107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the models you want to export reports for.\n",
    "# This saves full_config.yaml + config_summary.json in each model directory.\n",
    "\n",
    "MODELS_TO_REPORT = {\n",
    "    # (model_key, trained_model_dir, num_classes)\n",
    "    # AGAR models\n",
    "    \"total_faster_rcnn_R50\":  (\"faster_rcnn_R50\",  config.AGAR_TRAINED_MODELS[\"total_faster_rcnn_R50\"],  3),\n",
    "    \"total_faster_rcnn_R101\": (\"faster_rcnn_R101\", config.AGAR_TRAINED_MODELS[\"total_faster_rcnn_R101\"], 3),\n",
    "    \"total_retinanet_R50\":    (\"retinanet_R50\",    config.AGAR_TRAINED_MODELS[\"total_retinanet_R50\"],    3),\n",
    "    \"total_retinanet_R101\":   (\"retinanet_R101\",   config.AGAR_TRAINED_MODELS[\"total_retinanet_R101\"],   3),\n",
    "    \"total_mask_rcnn_R50\":    (\"mask_rcnn_R50\",    config.AGAR_TRAINED_MODELS[\"total_mask_rcnn_R50\"],    3),\n",
    "    \"total_mask_rcnn_R101\":   (\"mask_rcnn_R101\",   config.AGAR_TRAINED_MODELS[\"total_mask_rcnn_R101\"],   3),\n",
    "}\n",
    "\n",
    "for label, (model_key, model_dir, n_cls) in MODELS_TO_REPORT.items():\n",
    "    print(f\"\\n--- {label} ---\")\n",
    "    generate_reproducibility_report(model_key, model_dir, num_classes=n_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1370963b",
   "metadata": {},
   "source": [
    "---\n",
    "## Part B: Uncertainty Quantification (Bootstrap Confidence Intervals)\n",
    "\n",
    "**Reviewer concern:** *\"No uncertainty quantification (confidence intervals,\n",
    "variance estimates).\"*\n",
    "\n",
    "**Response:** We compute 95% bootstrap confidence intervals by resampling\n",
    "test images with replacement (N=1000) and running COCOeval on each resample.\n",
    "This quantifies the variability of AP due to the specific test-set composition\n",
    "without requiring retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5054d66",
   "metadata": {},
   "source": [
    "### B.1 Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b822251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== CONFIGURE =====================\n",
    "\n",
    "# Ground truth annotation file\n",
    "DATASET_SOURCE = \"agar\"\n",
    "SUBSET = \"total\"\n",
    "\n",
    "if DATASET_SOURCE == \"agar\":\n",
    "    GT_PATH = config.AGAR_DATASETS[SUBSET][\"test\"]\n",
    "else:\n",
    "    GT_PATH = config.ROBOFLOW_DATASETS[\"curated\"][\"test\"]\n",
    "\n",
    "# Prediction file from the model to analyze\n",
    "TRAINED_MODEL_KEY = \"total_faster_rcnn_R101\"\n",
    "MODEL_SOURCE = \"agar\"   # 'agar' or 'roboflow'\n",
    "PREDICTIONS_SUBFOLDER = \"0\"   # or 'test', '2', etc.\n",
    "\n",
    "PREDICTIONS_PATH = config.get_predictions_path(\n",
    "    TRAINED_MODEL_KEY, MODEL_SOURCE, PREDICTIONS_SUBFOLDER\n",
    ")\n",
    "\n",
    "# Bootstrap params\n",
    "N_BOOTSTRAP = 1000\n",
    "CONFIDENCE_LEVEL = 0.95\n",
    "SEED = 42\n",
    "\n",
    "# ====================================================\n",
    "\n",
    "print(f\"Ground truth: {GT_PATH}\")\n",
    "print(f\"Predictions:  {PREDICTIONS_PATH}\")\n",
    "print(f\"Bootstrap:    {N_BOOTSTRAP} iterations, {int(CONFIDENCE_LEVEL*100)}% CI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aab5ec",
   "metadata": {},
   "source": [
    "### B.2 Run Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91b83b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_results = bootstrap_coco_eval(\n",
    "    gt_path=GT_PATH,\n",
    "    predictions_path=PREDICTIONS_PATH,\n",
    "    n_bootstrap=N_BOOTSTRAP,\n",
    "    confidence_level=CONFIDENCE_LEVEL,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb46d8",
   "metadata": {},
   "source": [
    "### B.3 Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6774d143",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_table = format_ci_table(ci_results, confidence_level=CONFIDENCE_LEVEL)\n",
    "display(ci_table)\n",
    "\n",
    "# Save to CSV\n",
    "ci_output = os.path.join(config.RESULTS_DIR, \"bootstrap_ci\")\n",
    "os.makedirs(ci_output, exist_ok=True)\n",
    "csv_path = os.path.join(ci_output, f\"{TRAINED_MODEL_KEY}_bootstrap_ci.csv\")\n",
    "ci_table.to_csv(csv_path, index=False)\n",
    "print(f\"\\nSaved to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a1a6a0",
   "metadata": {},
   "source": [
    "### B.4 Distribution Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e287924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_save = os.path.join(ci_output, f\"{TRAINED_MODEL_KEY}_bootstrap_dist.png\")\n",
    "plot_bootstrap_distributions(ci_results, save_path=plot_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a890dda9",
   "metadata": {},
   "source": [
    "### B.5 Run for Multiple Models (Batch)\n",
    "\n",
    "Run bootstrap CIs for all models to build a comparison table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61741f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run batch bootstrap for all AGAR total models\n",
    "#\n",
    "# MODELS_TO_BOOTSTRAP = [\n",
    "#     \"total_faster_rcnn_R50\",\n",
    "#     \"total_faster_rcnn_R101\",\n",
    "#     \"total_retinanet_R50\",\n",
    "#     \"total_retinanet_R101\",\n",
    "#     \"total_mask_rcnn_R50\",\n",
    "#     \"total_mask_rcnn_R101\",\n",
    "# ]\n",
    "#\n",
    "# all_ci = {}\n",
    "# for key in MODELS_TO_BOOTSTRAP:\n",
    "#     pred_path = config.get_predictions_path(key, \"agar\", PREDICTIONS_SUBFOLDER)\n",
    "#     if not os.path.exists(pred_path):\n",
    "#         print(f\"Skipping {key}: {pred_path} not found\")\n",
    "#         continue\n",
    "#     print(f\"\\n{'='*50}\")\n",
    "#     print(f\"Model: {key}\")\n",
    "#     print(f\"{'='*50}\")\n",
    "#     all_ci[key] = bootstrap_coco_eval(\n",
    "#         GT_PATH, pred_path, n_bootstrap=N_BOOTSTRAP,\n",
    "#         confidence_level=CONFIDENCE_LEVEL, seed=SEED,\n",
    "#     )\n",
    "#\n",
    "# # Summary comparison table\n",
    "# import pandas as pd\n",
    "# rows = []\n",
    "# for key, res in all_ci.items():\n",
    "#     rows.append({\n",
    "#         \"Model\": key,\n",
    "#         \"AP (mean ± std)\": f\"{res['AP']['mean']:.4f} ± {res['AP']['std']:.4f}\",\n",
    "#         \"AP 95% CI\": f\"[{res['AP']['ci_low']:.4f}, {res['AP']['ci_high']:.4f}]\",\n",
    "#         \"AP50 (mean ± std)\": f\"{res['AP50']['mean']:.4f} ± {res['AP50']['std']:.4f}\",\n",
    "#     })\n",
    "# display(pd.DataFrame(rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b691a8",
   "metadata": {},
   "source": [
    "---\n",
    "## Part C: IoU Threshold Justification & Size Analysis\n",
    "\n",
    "**Reviewer concern:** *\"COCO mAP thresholds not justified for very\n",
    "small colonies.\"*\n",
    "\n",
    "**Response:** We (1) report the size distribution of colonies using COCO\n",
    "size categories, (2) evaluate at multiple individual IoU thresholds\n",
    "including a lenient 0.25 suitable for small objects, and (3) report\n",
    "AP-small, AP-medium, and AP-large separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23227e29",
   "metadata": {},
   "source": [
    "### C.1 Colony Size Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df114a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_save = os.path.join(config.RESULTS_DIR, \"size_analysis\")\n",
    "os.makedirs(size_save, exist_ok=True)\n",
    "\n",
    "size_stats = size_distribution_analysis(\n",
    "    gt_path=GT_PATH,\n",
    "    save_path=os.path.join(size_save, f\"{SUBSET}_size_distribution.png\"),\n",
    ")\n",
    "\n",
    "print(\"\\nSize Statistics:\")\n",
    "print(json.dumps(size_stats, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cc41fe",
   "metadata": {},
   "source": [
    "### C.2 Multi-Threshold Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433cdc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate at IoU = 0.25, 0.50, 0.75, 0.90\n",
    "# IoU=0.25 is included as a lenient threshold for small colonies\n",
    "\n",
    "mt_results = multi_threshold_evaluate(\n",
    "    gt_path=GT_PATH,\n",
    "    predictions_path=PREDICTIONS_PATH,\n",
    "    iou_thresholds=[0.25, 0.5, 0.75, 0.9],\n",
    "    per_category=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948a1db6",
   "metadata": {},
   "source": [
    "### C.3 Multi-Threshold Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2f841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_table = format_multi_threshold_table(mt_results)\n",
    "display(mt_table)\n",
    "\n",
    "# Save\n",
    "mt_csv = os.path.join(size_save, f\"{TRAINED_MODEL_KEY}_multi_iou.csv\")\n",
    "mt_table.to_csv(mt_csv, index=False)\n",
    "print(f\"\\nSaved to: {mt_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29f2a98",
   "metadata": {},
   "source": [
    "### C.4 Interpretation\n",
    "\n",
    "Key points for the reviewer response:\n",
    "\n",
    "- **Size distribution:** Report the percentage of colonies that fall in each\n",
    "  COCO size category. If most colonies are \"small\" (< 32² px), then the\n",
    "  standard IoU=0.5 threshold is indeed strict — even a few pixels of offset\n",
    "  on a 10×10 box drops IoU significantly.\n",
    "- **AP at IoU=0.25:** Provides a lenient evaluation suitable for small objects.\n",
    "- **AP-small vs AP-medium vs AP-large:** Shows how model performance varies\n",
    "  by colony size, directly addressing the reviewer's concern.\n",
    "- **Standard AP@[.50:.95]:** Remains the primary metric for comparability\n",
    "  with other work, but the additional thresholds provide context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ca589",
   "metadata": {},
   "source": [
    "---\n",
    "## Part D: Summary for Reviewer Letter\n",
    "\n",
    "Run this cell to generate a text summary suitable for copy-pasting into\n",
    "the reviewer response letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e44625",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"REVIEWER RESPONSE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# A: Config\n",
    "print(\"\\n[A] HYPERPARAMETERS & CONFIG\")\n",
    "print(\"  Full resolved Detectron2 YAML configs exported for all 6 architectures.\")\n",
    "print(\"  Key defaults (not overridden):\")\n",
    "s = extract_key_config_summary(\"faster_rcnn_R101\", num_classes=3)\n",
    "print(f\"    Anchors sizes:    {s['anchor_generator']['sizes']}\")\n",
    "print(f\"    Anchor ratios:    {s['anchor_generator']['aspect_ratios']}\")\n",
    "print(f\"    Pixel mean:       {s['input_preprocessing']['pixel_mean']}\")\n",
    "print(f\"    Pixel std:        {s['input_preprocessing']['pixel_std']}\")\n",
    "print(f\"    Input format:     {s['input_preprocessing']['format']}\")\n",
    "print(f\"    Min size train:   {s['input_preprocessing']['min_size_train']}\")\n",
    "print(f\"    Max size train:   {s['input_preprocessing']['max_size_train']}\")\n",
    "print(f\"    Augmentations:    {[a['name'] for a in s['data_augmentation_train']['augmentations']]}\")\n",
    "print(f\"    Random seed:      {s['random_seed']['seed']}\")\n",
    "\n",
    "# B: Bootstrap CIs\n",
    "print(\"\\n[B] UNCERTAINTY QUANTIFICATION\")\n",
    "print(f\"  Model: {TRAINED_MODEL_KEY}\")\n",
    "print(f\"  Bootstrap: {N_BOOTSTRAP} iterations, {int(CONFIDENCE_LEVEL*100)}% CI\")\n",
    "ap = ci_results['AP']\n",
    "print(f\"  AP:    {ap['mean']:.4f} ± {ap['std']:.4f}  \"\n",
    "      f\"95% CI [{ap['ci_low']:.4f}, {ap['ci_high']:.4f}]\")\n",
    "ap50 = ci_results['AP50']\n",
    "print(f\"  AP50:  {ap50['mean']:.4f} ± {ap50['std']:.4f}  \"\n",
    "      f\"95% CI [{ap50['ci_low']:.4f}, {ap50['ci_high']:.4f}]\")\n",
    "ap75 = ci_results['AP75']\n",
    "print(f\"  AP75:  {ap75['mean']:.4f} ± {ap75['std']:.4f}  \"\n",
    "      f\"95% CI [{ap75['ci_low']:.4f}, {ap75['ci_high']:.4f}]\")\n",
    "\n",
    "# C: Multi-threshold\n",
    "print(\"\\n[C] IoU THRESHOLD ANALYSIS\")\n",
    "for iou_thr, vals in sorted(mt_results.items()):\n",
    "    print(f\"  IoU={iou_thr}: AP={vals['AP']*100:.1f}%  \"\n",
    "          f\"AP-small={vals['AP_small']*100:.1f}%  \"\n",
    "          f\"AP-medium={vals['AP_medium']*100:.1f}%  \"\n",
    "          f\"AP-large={vals['AP_large']*100:.1f}%\")\n",
    "\n",
    "# D: Size\n",
    "print(\"\\n[D] COLONY SIZE DISTRIBUTION\")\n",
    "for bucket, count in size_stats['coco_size_buckets'].items():\n",
    "    pct = size_stats['coco_size_percentages'][bucket.split('(')[0].strip()]\n",
    "    print(f\"  {bucket}: {count} annotations ({pct})\")\n",
    "print(f\"  Median area: {size_stats['area']['median']:.1f} px²\")\n",
    "print(f\"  Mean area:   {size_stats['area']['mean']:.1f} px²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce124a8c",
   "metadata": {},
   "source": [
    "---\n",
    "## Part E: Filter Sensitivity Analysis (>100 annotations threshold)\n",
    "\n",
    "**Reviewer concern:** *Filtering images with >100 annotations could bias results.*\n",
    "\n",
    "**Response:** We analyze what the filter excludes — how many images/annotations\n",
    "are removed, and whether the remaining data has a meaningfully different\n",
    "size distribution. This shows the filter is a minor data-cleaning step,\n",
    "not a source of systematic bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c27f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import filter_sensitivity_analysis\n",
    "\n",
    "# ── Paths to UNFILTERED annotations ──\n",
    "# These are the annotations BEFORE the >100 filter was applied\n",
    "UNFILTERED_TRAIN = config.AGAR_UNFILTERED[\"total_train\"]\n",
    "\n",
    "sensitivity_save = os.path.join(config.RESULTS_DIR, \"filter_sensitivity\")\n",
    "os.makedirs(sensitivity_save, exist_ok=True)\n",
    "\n",
    "filter_stats = filter_sensitivity_analysis(\n",
    "    gt_path=UNFILTERED_TRAIN,\n",
    "    threshold=100,\n",
    "    save_path=os.path.join(sensitivity_save, \"filter_sensitivity_total.png\"),\n",
    ")\n",
    "\n",
    "print(\"\\nFilter Sensitivity Results:\")\n",
    "print(json.dumps(filter_stats, indent=2))\n",
    "\n",
    "# Save to JSON\n",
    "with open(os.path.join(sensitivity_save, \"filter_sensitivity_stats.json\"), 'w') as f:\n",
    "    json.dump(filter_stats, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5da70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sensitivity to different threshold values\n",
    "thresholds = [50, 75, 100, 150, 200]\n",
    "threshold_results = []\n",
    "\n",
    "for thr in thresholds:\n",
    "    stats = filter_sensitivity_analysis(\n",
    "        gt_path=UNFILTERED_TRAIN,\n",
    "        threshold=thr,\n",
    "        save_path=None,  # don't save individual plots\n",
    "    )\n",
    "    threshold_results.append({\n",
    "        \"Threshold\": thr,\n",
    "        \"Images kept\": stats[\"filtered\"][\"num_images\"],\n",
    "        \"Images excluded\": stats[\"excluded\"][\"num_images\"],\n",
    "        \"% excluded\": stats[\"excluded\"][\"pct_images_excluded\"],\n",
    "        \"Annotations kept\": stats[\"filtered\"][\"num_annotations\"],\n",
    "        \"Annotations excluded\": stats[\"excluded\"][\"num_annotations\"],\n",
    "        \"% anns excluded\": stats[\"excluded\"][\"pct_annotations_excluded\"],\n",
    "        \"Area shift (px²)\": f\"{stats['impact']['area_shift']:.1f}\",\n",
    "    })\n",
    "    plt.close('all')  # close the auto-generated plots\n",
    "\n",
    "import pandas as pd\n",
    "df_thr = pd.DataFrame(threshold_results)\n",
    "display(df_thr)\n",
    "\n",
    "csv_path = os.path.join(sensitivity_save, \"filter_threshold_comparison.csv\")\n",
    "df_thr.to_csv(csv_path, index=False)\n",
    "print(f\"\\nSaved to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7434785c",
   "metadata": {},
   "source": [
    "---\n",
    "## Part F: Integration of Multi-Seed & YOLOv8 Results\n",
    "\n",
    "**Prerequisites:** Run `10_multi_seed_train.ipynb` and `11_yolo_baseline.ipynb` first.\n",
    "\n",
    "This section loads the multi-seed training results and cross-architecture\n",
    "comparison to generate the final reviewer response tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee01fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ── Load multi-seed Detectron2 results ──\n",
    "multi_seed_path = os.path.join(config.RESULTS_DIR, \"multi_seed_results.json\")\n",
    "yolo_path = os.path.join(config.RESULTS_DIR, \"yolo_test_results.json\")\n",
    "\n",
    "if os.path.exists(multi_seed_path):\n",
    "    with open(multi_seed_path, 'r') as f:\n",
    "        d2_seed_results = json.load(f)\n",
    "    df_d2 = pd.DataFrame(d2_seed_results)\n",
    "    \n",
    "    print(\"=== Detectron2 Multi-Seed Results ===\")\n",
    "    for model in df_d2['model'].unique():\n",
    "        sub = df_d2[df_d2['model'] == model]\n",
    "        print(f\"\\n{model}:\")\n",
    "        for m in ['AP', 'AP50', 'AP75']:\n",
    "            mean = sub[m].mean()\n",
    "            std = sub[m].std()\n",
    "            print(f\"  {m}: {mean:.1f} ± {std:.1f}\")\n",
    "else:\n",
    "    print(f\"Multi-seed results not found: {multi_seed_path}\")\n",
    "    print(\"Run notebook 10 first.\")\n",
    "    d2_seed_results = None\n",
    "\n",
    "if os.path.exists(yolo_path):\n",
    "    with open(yolo_path, 'r') as f:\n",
    "        yolo_results = json.load(f)\n",
    "    df_yolo = pd.DataFrame(yolo_results)\n",
    "    \n",
    "    print(\"\\n=== YOLOv8 Results ===\")\n",
    "    for m in ['mAP50', 'mAP50_95', 'mAP75']:\n",
    "        mean = df_yolo[m].mean()\n",
    "        std = df_yolo[m].std()\n",
    "        print(f\"  {m}: {mean:.1f} ± {std:.1f}\")\n",
    "else:\n",
    "    print(f\"\\nYOLO results not found: {yolo_path}\")\n",
    "    print(\"Run notebook 11 first.\")\n",
    "    yolo_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e88a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if d2_seed_results and yolo_results:\n",
    "    rows = []\n",
    "    \n",
    "    # Detectron2 models\n",
    "    for model in df_d2['model'].unique():\n",
    "        sub = df_d2[df_d2['model'] == model]\n",
    "        rows.append({\n",
    "            \"Model\": model.replace(\"total_\", \"\"),\n",
    "            \"Framework\": \"Detectron2\",\n",
    "            \"AP (mean±std)\": f\"{sub['AP'].mean():.1f} ± {sub['AP'].std():.1f}\",\n",
    "            \"AP50 (mean±std)\": f\"{sub['AP50'].mean():.1f} ± {sub['AP50'].std():.1f}\",\n",
    "            \"AP75 (mean±std)\": f\"{sub['AP75'].mean():.1f} ± {sub['AP75'].std():.1f}\",\n",
    "            \"N seeds\": len(sub),\n",
    "        })\n",
    "    \n",
    "    # YOLO\n",
    "    rows.append({\n",
    "        \"Model\": \"YOLOv8m\",\n",
    "        \"Framework\": \"Ultralytics\",\n",
    "        \"AP (mean±std)\": f\"{df_yolo['mAP50_95'].mean():.1f} ± {df_yolo['mAP50_95'].std():.1f}\",\n",
    "        \"AP50 (mean±std)\": f\"{df_yolo['mAP50'].mean():.1f} ± {df_yolo['mAP50'].std():.1f}\",\n",
    "        \"AP75 (mean±std)\": f\"{df_yolo['mAP75'].mean():.1f} ± {df_yolo['mAP75'].std():.1f}\",\n",
    "        \"N seeds\": len(df_yolo),\n",
    "    })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(rows)\n",
    "    display(df_comparison)\n",
    "    \n",
    "    comp_csv = os.path.join(config.RESULTS_DIR, \"final_comparison_table.csv\")\n",
    "    df_comparison.to_csv(comp_csv, index=False)\n",
    "    print(f\"\\nSaved to: {comp_csv}\")\n",
    "else:\n",
    "    print(\"Cannot build comparison — run notebooks 10 and 11 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121d5a8f",
   "metadata": {},
   "source": [
    "### F.3 Updated Reviewer Response Summary\n",
    "\n",
    "With multi-seed and cross-architecture results available, the response\n",
    "to reviewers now includes:\n",
    "\n",
    "1. **[A] Full configs** — resolved YAML for all 6 architectures\n",
    "2. **[B] Bootstrap CIs** — evaluation-level uncertainty\n",
    "3. **[C] IoU thresholds** — AP@0.25/0.50/0.75/0.90 with size breakdown\n",
    "4. **[D] Colony sizes** — COCO size bucket distribution\n",
    "5. **[E] Filter sensitivity** — impact of >100 annotation threshold\n",
    "6. **[F] Training variance** — mean ± std across 3 seeds (Detectron2 + YOLOv8)\n",
    "\n",
    "Copy the tables from Parts B, C, E, and F into the reviewer letter."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
